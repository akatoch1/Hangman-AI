{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def build_dictionary(dictionary_file_location):\n",
        "  text_file = open(dictionary_file_location,\"r\")\n",
        "  full_dictionary = text_file.read().splitlines()\n",
        "  text_file.close()\n",
        "  return full_dictionary"
      ],
      "metadata": {
        "id": "mOPOdn7hu8Z9"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = build_dictionary(\"words_250000_train.txt\")"
      ],
      "metadata": {
        "id": "sk--OZC16vDI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def most_common_character_by_length(words):\n",
        "    words_by_length = defaultdict(list)\n",
        "    for word in words:\n",
        "        words_by_length[len(word)].append(word)\n",
        "\n",
        "    most_common_char_by_length = {}\n",
        "    for length, word_list in words_by_length.items():\n",
        "        char_counter = Counter()\n",
        "        for word in word_list:\n",
        "            char_counter.update(word)\n",
        "        most_common_char_by_length[length] = char_counter.most_common(1)[0][0]\n",
        "\n",
        "    return most_common_char_by_length"
      ],
      "metadata": {
        "id": "-qbjpCPM7yjD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KkfeteUu-Itl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pFmS4fQzt7St"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import string\n",
        "\n",
        "ALPHABET = string.ascii_lowercase\n",
        "ALPHABET_IDX = {char: idx+1 for idx, char in enumerate(ALPHABET)}\n",
        "ALPHABET_IDX['_'] = 0\n",
        "words = build_dictionary(\"words_250000_train.txt\")\n",
        "MAX_WORD_LENGTH = max([len(word) for word in words])\n",
        "\n",
        "class HangmanDataset(Dataset):\n",
        "    def __init__(self, words):\n",
        "        self.samples = []\n",
        "        for word in words:\n",
        "            self.samples.extend(self.create_samples(word))\n",
        "        self.samples = random.sample(self.samples, 10000)\n",
        "    def create_samples(self, word):\n",
        "        samples = []\n",
        "        word_indices = [ALPHABET_IDX[char] for char in word]\n",
        "        for i in range(1, len(word)):  # Start from 1 to ensure at least one letter is guessed\n",
        "            guessed_indices = word_indices[:i]\n",
        "            guessed_mask = [1 if idx in guessed_indices else 0 for idx in range(len(ALPHABET))]\n",
        "            input_state = [idx if idx in guessed_indices else ALPHABET_IDX['_'] for idx in word_indices]\n",
        "            remaining_letters = list(set(word_indices) - set(guessed_indices))\n",
        "            if remaining_letters:\n",
        "                target_letter = random.choice(remaining_letters)\n",
        "                samples.append((input_state, guessed_mask, target_letter))\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_state, guessed_mask, target_letter = self.samples[idx]\n",
        "        input_state = input_state + [ALPHABET_IDX['_']] * (MAX_WORD_LENGTH - len(input_state))  # Padding\n",
        "        return (torch.tensor(input_state, dtype=torch.long),\n",
        "                torch.tensor(guessed_mask, dtype=torch.float),\n",
        "                torch.tensor(target_letter, dtype=torch.long))\n",
        "\n",
        "# Example of how to use the dataset\n",
        "def build_dictionary(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        words = f.read().splitlines()\n",
        "    return words\n",
        "\n",
        "\n",
        "dataset = HangmanDataset(words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = .8\n",
        "train_size = int(train_ratio * len(dataset))\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])"
      ],
      "metadata": {
        "id": "8Wr_SeWyfEF_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)"
      ],
      "metadata": {
        "id": "vvjb2H36f83V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HangmanLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super(HangmanLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=ALPHABET_IDX['_'])\n",
        "        self.lstm = nn.LSTM(embedding_dim + 26, hidden_dim, num_layers = 2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, guessed):\n",
        "        x = self.embedding(x)\n",
        "        guessed = guessed.unsqueeze(1).expand(-1, x.size(1), -1)\n",
        "        x = torch.cat((x, guessed), dim = 2)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1])\n",
        "        return out\n",
        "\n",
        "\n",
        "INPUT_DIM = len(ALPHABET) + 1\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(ALPHABET) + 1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = HangmanLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n"
      ],
      "metadata": {
        "id": "477lM53Rt84m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M66Ms2Nu7nqu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "q-DC2yRsJ5gE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, criterion, optimizer, device, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for input_state, guessed_mask, target_letter in data_loader:\n",
        "            input_state, guessed_mask, target_letter = input_state.to(device), guessed_mask.to(device), target_letter.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_state, guessed_mask)\n",
        "            loss = criterion(output, target_letter)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(data_loader):.4f}')\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvP-wYRruUVH",
        "outputId": "9fb50a77-0ff6-4733-ed15-5d0dc5dc7eb5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 2.9322\n",
            "Epoch 2/5, Loss: 2.7871\n",
            "Epoch 3/5, Loss: 2.7128\n",
            "Epoch 4/5, Loss: 2.6532\n",
            "Epoch 5/5, Loss: 2.6054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_letter(model, current_word, guessed_letters, device):\n",
        "    model.eval()\n",
        "    word_indices = [ALPHABET_IDX[char] if char != '_' else ALPHABET_IDX['_'] for char in current_word]\n",
        "    guessed_mask = [1 if char in guessed_letters else 0 for char in ALPHABET]\n",
        "\n",
        "    word_indices += [ALPHABET_IDX['_']] * (MAX_WORD_LENGTH - len(word_indices))\n",
        "\n",
        "    input_state = torch.tensor([word_indices], dtype=torch.long).to(device)\n",
        "    guessed_mask = torch.tensor([guessed_mask], dtype=torch.float).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_state, guessed_mask)\n",
        "    predicted_letter_idx = output.argmax(dim=1).item()\n",
        "    predicted_letter = ALPHABET[predicted_letter_idx]\n",
        "\n",
        "    return predicted_letter\n",
        "\n",
        "current_word = \"ex_mple\"\n",
        "guessed_letters = ['e', 'x', 'm', 'p', 'l']\n",
        "predicted_letter = predict_next_letter(model, current_word, guessed_letters, device)\n",
        "print(predicted_letter)"
      ],
      "metadata": {
        "id": "75Z23-XIJSO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf230eb4-08e6-46a1-b840-50eefd7deb27"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "with torch.no_grad():  # No need to track gradients for testing\n",
        "    for inputs, guessed, targets in test_loader:\n",
        "\n",
        "        outputs = model(inputs, guessed)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_predictions += targets.size(0)\n",
        "        correct_predictions += (predicted == targets).sum().item()\n",
        "\n",
        "# Calculate the average loss and accuracy\n",
        "average_test_loss = test_loss / len(test_loader)\n",
        "accuracy = correct_predictions / total_predictions\n",
        "print(accuracy)\n",
        "print(average_test_loss)"
      ],
      "metadata": {
        "id": "xprsP6vj91bJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86212cfa-2d63-4185-c64b-c7b7a7f78757"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.167\n",
            "2.682900443673134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zw837UbqpfOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}