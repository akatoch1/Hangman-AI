# Hangman-AI

Still working on improving the model. I've seen people use n-grams to solve hangman. I'm experimenting with purely an LSTM approach to the problem. Currently have the input as the word state which is encoded into a tensor with characters mapped to numbers as well as the underscore (unguessed letters). Guessed letters are masked and concatenated to the input to help the model understand what has been guessed so far and try and push the model to predict letters that have not been predicted. The reason to use LSTM is because playing hangman can be treated as a sequence. Our next predicted letter will continue the sequence and depends on our previously guessed correct and incorrect answers. Another possible way to structure the data would be to permute the order of which we could have correct guesses and predict letters based on that sequence. Example: 'car' -> c a r, a r c, r c a, c r a... and so on. Random amount of letters would be masked for prediction. Leave a comment if you have any other cool ideas
